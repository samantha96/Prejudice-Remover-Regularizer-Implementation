# Prejudice-Remover-Regularizer-Implementation


## Implementation PR with Pytorch

The use of data mining technologies in our society will only become greater and greater. Unfortunately, their results can occasionally damage people's lives through some socially sensitive information: gender, religion, race. On the other hand, data analysis is crucial for enhancing public welfare. For example, exploiting personal information has proved to be effective for reducing energy consumption, improving the eciency of trac control, preventing infectious diseases, and so on. Consequently, methods of data exploitation that do not damage people's lives, such as fairness/discrimination-aware learning, privacy preserving data mining, or adversarial learning, together comprise the notion of socially responsible mining, which it should become an important concept in the near future.

In this paper, mutual information between target class and sensitive variable was added to the object fuction as a regularizer. Using negative log likelihood maximum to calculate the loss function for logistic regression model. Sensitive class is gender(male, female)

•Applications

•Difficulty in Fairness-aware Data Mining

•Fairness-aware Classification

•prejudice remover regularizer, Calders-Verwer’s 2-naïve-Bayes

•Experiments

•experimental results on Calders&Verwer’s data and synthetic data

•Related Work



